---
title: "Model - Yule Simon for Component Distribution"
author: "Gabrielle Lemire"
date: "3/12/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(rlist)
library(reshape2)
library(rstanarm)
library(boot)
library(poweRlaw) #compare dists
library(GLDEX) #remove zeros
# load data
# load objects created in "sampling_sim.R"
sim_dist_100 = read.csv("C://Code//msu//gra//BayesNet//Paper8//sim_dists.csv")

# --------------- MELT ALL PROPORTIONS
melt_all_props = function(data = sim_dist_100){
  library(reshape2)
  # create one master df with the following columns:
  # X - the size of the component
  # variable - which sim it came from
  # value - count of components
  
  beg_prop = seq(from = 1, to = 346, by = 23)
  end_prop = seq(from = 23, to = 368, by = 23)
  samp_prop = seq(from = .95, to = 0.2, by = -0.05)
  df = data.frame()
  df_all_sim = data.frame()
  for (i in 1:length(beg_prop)){
    beg = beg_prop[i]
    end = end_prop[i]
    df = as.data.frame(data[beg:end,])
    df$X = c(1:23) #leave as 1 to 23 because all props should have this component count
    df = melt(df, id.vars="X")
    df_all_sim = rbind(df_all_sim, df)
  }
  return(df_all_sim)
}

# melt dataframe with all proportions and simulations into one master df_all_sim
df_all_sim = melt_all_props(data = sim_dist_100)

# Data for one simulation of prop=.95
data_1 = df_all_sim[1:23,3]
data_1 = data_1[order(-data_1)] #wait why are we ordering this?

# Data for one simulation of prop=.95 adding 1's
data_2 = df_all_sim[1:23,3] + 1
data_2 = data_2[order(-data_2)] #wait why are we ordering this?
```

## Overview of work in this File

While discussing results from running comparisons with the poweRlaw package between four distributions (power law, log normal, exponential and poisson) we found that power law and log normal both fit different parts of the curve better and exponential and poisson have a poor fit. We want to see if adding another degree of freedom will allow the fit of the curve to be better and want to explore the option of Yule-Simon. 

<!-- two params - combine models? -->
<!-- ex Yule Simon (extra degree of freedom might be enough to capture) -->
<!-- can we add the extra data... add simulations -->
<!-- how to add simulations?... try to do it just add them as extra points -->

## Yule-Simon Model for Component Distribution

```{r, echo=FALSE}
library(VGAM)
data = data.frame(comp = 1:23, count = data_2)
# from "yulesimon" example in cran docs:
model = VGAM::vglm(count ~ comp, yulesimon, data = data, trace = TRUE)
# https://www.rdocumentation.org/packages/VGAM/versions/0.9-0/topics/yulesimon
coef(model, matrix = TRUE)
summary(model)
VGAM::plotvglm(model)
head(fitted(model)) #why are these all NAs?


# https://www.rdocumentation.org/packages/VGAM/versions/0.8-3/topics/predict.vglm
data$predict = predict(model, newdata=data$count, type="response")


# easier to see on actual vs predicted curve -- create that
# are they unstandardized pearson residuals (in image), so will depend on link function
# it might be an improvement because residuals on either side
# looks promising
# because no scale of how on/off it is (compare to others)

# everything suggests just rho so not sure what two parameters do to it?
```

## Build Model in Loop - WIP

```{r}

data_2 = df_all_sim[1:23,3] + 1
data_2 = data_2[order(-data_2)]
data = data.frame(comp = 1:23, count = data_2)

df_all_sim

# --- get first simulation from each proportion to build model with
data_comp = data.frame(comp=1:23)
beg_sim_line = seq(from = 1, to = 36800, by = 2300)
end_sim_line = seq(from = 23, to = 36800, by = 2300)
samp_prop = seq(from = .95, to = 0.2, by = -0.05)
samp_prop_names = c("ninefive", "nine", "eightfive", "eight", "sevenfive", "seven", "sixfive", "six", "fivefive", "five", "fourfive", "four", "threefive", "three", "twofive", "two")

# can start out by just calling and storing certain things about the model
# store diagnostic plots etc

for (i in 1:16){
  # library(reshape2)
  beg = beg_sim_line[i]
  end = end_sim_line[i]
  data_comp[as.character(samp_prop_names[i])] = df_all_sim[beg:end,3] + 1
}
# --- run model on first simulation from each proportion
models_comp <- list()
# i=1
for (i in 1:16) {
  models_comp[as.character(samp_prop_names[i])]<-  
    # lm(y ~., data.frame(poly(x, i)))
    #     print(summary(mod[[i]]))
}

VGAM::vglm(as.character(0.95) ~ comp, yulesimon, data = data_comp, trace = TRUE)
VGAM::vglm(as.character(samp_prop[i]) ~ comp, yulesimon, data = data_comp, trace = TRUE)

```


## Yule-Simon Model for True Dist (Old Code)

revisit below code

```{r, echo=FALSE}
library(VGAM)

# yulesimon(link = "loge", irho = NULL, nsimEIM = 200, zero = NULL)
# https://www.rdocumentation.org/packages/VGAM/versions/0.9-7/topics/yulesimon
# drop 0's got error with them -- probably will want to add 1's to all
true_temp = true_dist_1[c(1,2,3,4,5,6,7,8,9, 11, 12, 14, 20, 23),]
# by hand - will need to do by loop
df_true = as.data.frame(matrix(nrow=sum(true_temp$x), ncol=1))
df_true[1:659,1]=1
df_true[(659+1):(659+102),1]=2
df_true[(761+1):(761+24),1]=3
df_true[(785+1):(785+15),1]=4
df_true[(800+1):(800+9),1]=5
df_true[(809+1):(809+4),1]=6
df_true[(813+1):(813+3),1]=7
df_true[(816+1):(816+3),1]=8
df_true[(819+1):(819+4),1]=9
df_true[(823+1):(823+1),1]=11
df_true[(824+1):(824+2),1]=12
df_true[(826+1):(826+1),1]=14
df_true[(827+1):(827+1),1]=20
df_true[(828+1):(828+1),1]=23

model <- vglm(x ~ X, yulesimon, data = true_temp, trace = TRUE)
# coef(model, matrix = TRUE)
summary(model)

# https://www.rdocumentation.org/packages/VGAM/versions/0.8-3/topics/predict.vglm
true_temp$predict = predict(model, newdata=true_temp$x, type="response") # lol results make no sense
#maybe predicting on link scale

```

**Notes from group meeting on May 21, 2021**: 

tfidf - zipfl law freq vs rank... common for text analytics

-like an indicator __species
-idf is inverse document relative freqncies (idf) -- usually cast as natual log of the ratio of # of docs in corpus / num of docs that contain the term... but if nothing then it contains a 0 ... so ppl will replace with a 1... but th way he teaches it ... polyamorous - a very common term that appears many times in many docs, and the upper extreme is going to be bounded by the ln(idf), and that term would be an indicator term or monogamous (very special and a characteristic signature for that corpus)...

- does it maintain dist, do params change, etc as sample rate changes
- in survival analysis - exp, weibull ,ln, etc... typical sop is likelihood ratio to compare pairs & proc life reg for gof of each model
prob/proc __ metric -- to see what fit is like... generates a prob-prob or quant-quant plot, uses a mod of Kaplan-Meyer to compare points to line...
- Nicole - sow e'd be thinking of cluster size as survival time... 

- do we care about drop off of 1's
- diff sim's

**old notes can probably delete but I'm a hoarder**:

-give understanding of yule-simon family next meeting (what is a vglm, etc) & how it relates to glm/what i've seen already

- big question after will be how to move parameters in this situation into bigger epidemic model